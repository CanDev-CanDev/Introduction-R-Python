{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$.getScript('http://asimjalis.github.io/ipyn-ext/js/ipyn-present.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- \n",
    "The ipynb was auto-generated from markdown using notedown.\n",
    "Instead of modifying the ipynb file modify the markdown source. \n",
    "-->\n",
    "\n",
    "<h1 class=\"tocheading\">Spark</h1>\n",
    "<div id=\"toc\"></div>\n",
    "\n",
    "<img src=\"images/spark-logo.png\">\n",
    "\n",
    "Apache Spark\n",
    "============\n",
    "\n",
    "Spark Intro\n",
    "-----------\n",
    "\n",
    "What is Spark?\n",
    "\n",
    "- Spark is a framework for distributed processing.\n",
    "\n",
    "- It is a streamlined alternative to Map-Reduce.\n",
    "\n",
    "- Spark applications can be written in Python, Scala, or Java.\n",
    "\n",
    "Why Spark\n",
    "---------\n",
    "\n",
    "Why learn Spark?\n",
    "\n",
    "- Spark enables you to analyze petabytes of data.\n",
    "\n",
    "- Spark skills are in high demand--<http://indeed.com/salary>.\n",
    "\n",
    "- Spark is signficantly faster than MapReduce.\n",
    "\n",
    "- Paradoxically, Spark's API is simpler than the MapReduce API.\n",
    "\n",
    "Goals\n",
    "-----\n",
    "\n",
    "By the end of this lecture, you will be able to:\n",
    "\n",
    "- Create RDDs to distribute data across a cluster\n",
    "\n",
    "- Use the Spark shell to compose and execute Spark commands\n",
    "\n",
    "- Use Spark to analyze stock market data\n",
    "\n",
    "Spark Version History\n",
    "---------------------\n",
    "\n",
    "Date               |Version        |Changes\n",
    "----               |-------        |-------\n",
    "May 30, 2014       |Spark 1.0.0    |APIs stabilized \n",
    "September 11, 2014 |Spark 1.1.0    |New functions in MLlib, Spark SQL\n",
    "December 18, 2014  |Spark 1.2.0    |Python Streaming API and better streaming fault tolerance\n",
    "March 13, 2015     |Spark 1.3.0    |DataFrame API, Kafka integration in Streaming\n",
    "April 17, 2015     |Spark 1.3.1    |Bug fixes, minor changes\n",
    "\n",
    "Matei Zaharia\n",
    "-------------\n",
    "\n",
    "<img style=\"width:50%\" src=\"images/matei.jpg\">\n",
    "\n",
    "Essense of Spark\n",
    "----------------\n",
    "\n",
    "What is the basic idea of Spark?\n",
    "\n",
    "- Spark takes the Map-Reduce paradigm and changes it in some critical\n",
    "  ways.\n",
    "\n",
    "- Instead of writing single Map-Reduce jobs a Spark job consists of a\n",
    "  series of map and reduce functions. \n",
    "  \n",
    "- However, the intermediate data is kept in memory instead of being\n",
    "  written to disk or written to HDFS.\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: Since Spark keeps intermediate data in memory to get speed, what\n",
    "does it make us give up? Where's the catch?\n",
    "</summary>\n",
    "1. Spark does a trade-off between memory and performance.\n",
    "<br>\n",
    "2. While Spark apps are faster, they also consume more memory.\n",
    "<br>\n",
    "3. Spark outshines Map-Reduce in iterative algorithms where the\n",
    "   overhead of saving the results of each step to HDFS slows down\n",
    "   Map-Reduce.\n",
    "<br>\n",
    "4. For non-iterative algorithms Spark is comparable to Map-Reduce.\n",
    "</details>\n",
    "\n",
    "Spark Logging\n",
    "-------------\n",
    "\n",
    "Q: How can I make Spark logging less verbose?\n",
    "\n",
    "- By default Spark logs messages at the `INFO` level.\n",
    "\n",
    "- Here are the steps to make it only print out warnings and errors.\n",
    "\n",
    "```sh\n",
    "cd $SPARK_HOME/conf\n",
    "cp log4j.properties.template log4j.properties\n",
    "```\n",
    "\n",
    "- Edit `log4j.properties` and replace `rootCategory=INFO` with `rootCategory=ERROR`\n",
    "\n",
    "Spark Fundamentals\n",
    "==================\n",
    "\n",
    "Spark Execution\n",
    "---------------\n",
    "\n",
    "<img src=\"images/spark-cluster.png\">\n",
    "\n",
    "\n",
    "Spark Terminology\n",
    "-----------------\n",
    "\n",
    "Term                   |Meaning\n",
    "----                   |-------\n",
    "Driver                 |Process that contains the Spark Context\n",
    "Executor               |Process that executes one or more Spark tasks\n",
    "Master                 |Process which manages applications across the cluster\n",
    "                       |E.g. Spark Master\n",
    "Worker                 |Process which manages executors on a particular worker node\n",
    "                       |E.g. Spark Worker\n",
    "\n",
    "Spark Job\n",
    "---------\n",
    "\n",
    "Q: Flip a coin 100 times using Python's `random()` function. What\n",
    "fraction of the time do you get heads?\n",
    "\n",
    "- Initialize Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "flips = 1000000\n",
    "heads = sc.parallelize(xrange(flips)) \\\n",
    "    .map(lambda i: random.random()) \\\n",
    "    .filter(lambda r: r < 0.51) \\\n",
    "    .count()\n",
    "\n",
    "ratio = float(heads)/float(flips)\n",
    "\n",
    "print(heads)\n",
    "print(ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes\n",
    "-----\n",
    "\n",
    "- `sc.parallelize` creates an RDD.\n",
    "\n",
    "- `map` and `filter` are *transformations*.\n",
    "\n",
    "- They create new RDDs from existing RDDs.\n",
    "\n",
    "- `count` is an *action* and brings the data from the RDDs back to the\n",
    "  driver.\n",
    "\n",
    "Spark Terminology\n",
    "-----------------\n",
    "\n",
    "Term                   |Meaning\n",
    "----                   |-------\n",
    "RDD                    |*Resilient Distributed Dataset* or a distributed sequence of records\n",
    "Spark Job              |Sequence of transformations on data with a final action\n",
    "Spark Application      |Sequence of Spark jobs and other code\n",
    "Transformation         |Spark operation that produces an RDD\n",
    "Action                 |Spark operation that produces a local object\n",
    "\n",
    "\n",
    "- A Spark job consists of a series of transformations followed by an\n",
    "  action.\n",
    "\n",
    "- It pushes the data to the cluster, all computation happens on the\n",
    "  *executors*, then the result is sent back to the driver.\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "In this Spark job what is the transformation is what is the action? \n",
    "`sc.parallelize(xrange(10)).filter(lambda x: x % 2 == 0).collect()`\n",
    "</summary>\n",
    "1. `filter` is the transformation.\n",
    "<br>\n",
    "2. `collect` is the action.\n",
    "</details>\n",
    "\n",
    "Lambda vs Functions\n",
    "-------------------\n",
    "\n",
    "- Instead of `lambda` you can pass in fully defined functions into\n",
    "  `map`, `filter`, and other RDD transformations.\n",
    "\n",
    "- Use `lambda` for short functions. \n",
    "\n",
    "- Use `def` for more substantial functions.\n",
    "\n",
    "Finding Primes\n",
    "--------------\n",
    "\n",
    "Q: Find all the primes less than 100.\n",
    "\n",
    "- Define function to determine if a number is prime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_prime(number):\n",
    "    factor_min = 2\n",
    "    factor_max = int(number**0.5)+1\n",
    "    for factor in xrange(factor_min,factor_max):\n",
    "        if number % factor == 0:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use this to filter out non-primes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numbers = xrange(2,100)\n",
    "primes = sc.parallelize(numbers)\\\n",
    "    .filter(is_prime)\\\n",
    "    .collect()\n",
    "print primes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<img src=\"images/spark-cluster.png\">\n",
    "\n",
    "<details><summary>\n",
    "Q: Where does `is_prime` execute?\n",
    "</summary>\n",
    "On the executors.\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "Q: Where does the RDD code execute?\n",
    "</summary>\n",
    "On the driver.\n",
    "</details>\n",
    "\n",
    "Transformations and Actions\n",
    "===========================\n",
    "\n",
    "Common RDD Constructors\n",
    "-----------------------\n",
    "\n",
    "Expression                               |Meaning\n",
    "----------                               |-------\n",
    "`sc.parallelize(list1)`                  |Create RDD of elements of list\n",
    "`sc.textFile(path)`                      |Create RDD of lines from file\n",
    "\n",
    "Common Transformations\n",
    "----------------------\n",
    "\n",
    "Expression                               |Meaning\n",
    "----------                               |-------\n",
    "`filter(lambda x: x % 2 == 0)`           |Discard non-even elements\n",
    "`map(lambda x: x * 2)`                   |Multiply each RDD element by `2`\n",
    "`map(lambda x: x.split())`               |Split each string into words\n",
    "`flatMap(lambda x: x.split())`           |Split each string into words and flatten sequence\n",
    "`sample(withReplacement=True,0.25)`      |Create sample of 25% of elements with replacement\n",
    "`union(rdd)`                             |Append `rdd` to existing RDD\n",
    "`distinct()`                             |Remove duplicates in RDD\n",
    "`sortBy(lambda x: x, ascending=False)`   |Sort elements in descending order\n",
    "\n",
    "\n",
    "Common Actions\n",
    "--------------\n",
    "\n",
    "Expression                             |Meaning\n",
    "----------                             |-------\n",
    "`collect()`                            |Convert RDD to in-memory list \n",
    "`take(3)`                              |First 3 elements of RDD \n",
    "`top(3)`                               |Top 3 elements of RDD\n",
    "`takeSample(withReplacement=True,3)`   |Create sample of 3 elements with replacement\n",
    "`sum()`                                |Find element sum (assumes numeric elements)\n",
    "`mean()`                               |Find element mean (assumes numeric elements)\n",
    "`stdev()`                              |Find element deviation (assumes numeric elements)\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "Q: What will this output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.parallelize([1,3,2,2,1]).distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: What will this output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.parallelize([1,3,2,2,1]).sortBy(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: What will this output?\n",
    "\n",
    "- Create this input file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile input.txt\n",
    "hello world\n",
    "another line\n",
    "yet another line\n",
    "yet another another line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What do you get when you run this code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.textFile('input.txt') \\\n",
    "    .map(lambda x: x.split()) \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What about this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.textFile('input.txt') \\\n",
    "    .flatMap(lambda x: x.split()) \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map vs FlatMap\n",
    "--------------\n",
    "\n",
    "- Here's the difference between `map` and `flatMap`.\n",
    "\n",
    "- Map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.textFile('input.txt') \\\n",
    "    .map(lambda x: x.split()) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- FlatMap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.textFile('input.txt') \\\n",
    "    .flatMap(lambda x: x.split()) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Value Pairs\n",
    "===============\n",
    "\n",
    "PairRDD\n",
    "-------\n",
    "\n",
    "At this point we know how to aggregate values across an RDD. If we\n",
    "have an RDD containing sales transactions we can find the total\n",
    "revenue across all transactions.\n",
    "\n",
    "Q: Using the following sales data find the total revenue across all\n",
    "transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile sales.txt\n",
    "#ID    Date           Store   State  Product    Amount\n",
    "101    11/13/2014     100     WA     331        300.00\n",
    "104    11/18/2014     700     OR     329        450.00\n",
    "102    11/15/2014     203     CA     321        200.00\n",
    "106    11/19/2014     202     CA     331        330.00\n",
    "103    11/17/2014     101     WA     373        750.00\n",
    "105    11/19/2014     202     CA     321        200.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split the lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove `#`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: x[0].startswith('#'))\\\n",
    "    .take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pick off last field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: x[-1])\\\n",
    "    .take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convert to float and then sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: float(x[-1]))\\\n",
    "    .sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReduceByKey\n",
    "-----------\n",
    "\n",
    "Q: Calculate revenue per state?\n",
    "\n",
    "- Instead of creating a sequence of revenue numbers we can create\n",
    "  tuples of states and revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: (x[-3],float(x[-1])))\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now use `reduceByKey` to add them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: (x[-3],float(x[-1])))\\\n",
    "    .reduceByKey(lambda amount1,amount2: amount1+amount2)\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Find the state with the highest total revenue.\n",
    "\n",
    "- You can either use the action `top` or the transformation `sortBy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: (x[-3],float(x[-1])))\\\n",
    "    .reduceByKey(lambda amount1,amount2: amount1+amount2)\\\n",
    "    .sortBy(lambda state_amount:state_amount[1],ascending=False) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: What does `reduceByKey` do?\n",
    "</summary>\n",
    "1. It is like a reducer.\n",
    "<br>\n",
    "2. If the RDD is made up of key-value pairs, it combines the values\n",
    "   across all tuples with the same key by using the function we pass\n",
    "   to it.\n",
    "<br>\n",
    "3. It only works on RDDs made up of key-value pairs or 2-tuples.\n",
    "</details>\n",
    "\n",
    "Notes\n",
    "-----\n",
    "\n",
    "- `reduceByKey` only works on RDDs made up of 2-tuples.\n",
    "\n",
    "- `reduceByKey` works as both a reducer and a combiner.\n",
    "\n",
    "- It requires that the operation is associative.\n",
    "\n",
    "Word Count\n",
    "----------\n",
    "\n",
    "Q: Implement word count in Spark.\n",
    "\n",
    "- Create some input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile input.txt\n",
    "hello world\n",
    "another line\n",
    "yet another line\n",
    "yet another another line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Count the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.textFile('input.txt')\\\n",
    "    .flatMap(lambda line: line.split())\\\n",
    "    .map(lambda word: (word,1))\\\n",
    "    .reduceByKey(lambda count1,count2: count1+count2)\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making List Indexing Readable\n",
    "-----------------------------\n",
    "\n",
    "- While this code looks reasonable, the list indexes are cryptic and\n",
    "  hard to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: (x[-3],float(x[-1])))\\\n",
    "    .reduceByKey(lambda amount1,amount2: amount1+amount2)\\\n",
    "    .sortBy(lambda state_amount:state_amount[1],ascending=False) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can make this more readable using Python's argument unpacking\n",
    "  feature.\n",
    "\n",
    "Argument Unpacking\n",
    "------------------\n",
    "\n",
    "Q: Which version of `getCity` is more readable and why?\n",
    "\n",
    "- Consider this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = ('Dmitri','Smith','SF')\n",
    "\n",
    "def getCity1(client):\n",
    "    return client[2]\n",
    "\n",
    "def getCity2((first,last,city)):\n",
    "    return city\n",
    "\n",
    "print getCity1(client)\n",
    "\n",
    "print getCity2(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is the difference between `getCity1` and `getCity2`?\n",
    "\n",
    "- Which is more readable?\n",
    "\n",
    "- What is the essence of argument unpacking?\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "<details><summary>\n",
    "Q: Can argument unpacking work for deeper nested structures?\n",
    "</summary>\n",
    "Yes. It can work for arbitrarily nested tuples and lists.\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "Q: How would you write `getCity` given \n",
    "`client = ('Dmitri','Smith',('123 Eddy','SF','CA'))`\n",
    "</summary>\n",
    "`def getCity((first,last,(street,city,state))): return city`\n",
    "</details>\n",
    "\n",
    "Argument Unpacking\n",
    "------------------\n",
    "\n",
    "- Lets test this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = ('Dmitri','Smith',('123 Eddy','SF','CA'))\n",
    "\n",
    "def getCity((first,last,(street,city,state))):\n",
    "    return city\n",
    "\n",
    "getCity(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Whenever you find yourself indexing into a tuple consider using\n",
    "  argument unpacking to make it more readable.\n",
    "\n",
    "- Here is what `getCity` looks like with tuple indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def badGetCity(client):\n",
    "    return client[2][1]\n",
    "\n",
    "getCity(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument Unpacking In Spark\n",
    "---------------------------\n",
    "\n",
    "Q: Rewrite the last Spark job using argument unpacking.\n",
    "\n",
    "- Here is the original version of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: (x[-3],float(x[-1])))\\\n",
    "    .reduceByKey(lambda amount1,amount2: amount1+amount2)\\\n",
    "    .sortBy(lambda state_amount:state_amount[1],ascending=False) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here is the code with argument unpacking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda (id,date,store,state,product,amount): (state,float(amount)))\\\n",
    "    .reduceByKey(lambda amount1,amount2: amount1+amount2)\\\n",
    "    .sortBy(lambda (state,amount):amount,ascending=False) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this case because we have a long list or tuple argument unpacking\n",
    "  is a judgement call.\n",
    "\n",
    "GroupByKey\n",
    "----------\n",
    "\n",
    "`reduceByKey` lets us aggregate values using sum, max, min, and other\n",
    "associative operations. But what about non-associative operations like\n",
    "average? How can we calculate them?\n",
    "\n",
    "- There are several ways to do this.\n",
    "\n",
    "- The first approach is to change the RDD tuples so that the operation\n",
    "  becomes associative. \n",
    "\n",
    "- Instead of `(state, amount)` use `(state, (amount, count))`.\n",
    "\n",
    "- The second approach is to use `groupByKey`, which is like\n",
    "  `reduceByKey` except it gathers together all the values in an\n",
    "  iterator. \n",
    "  \n",
    "- The iterator can then be reduced in a `map` step immediately after\n",
    "  the `groupByKey`.\n",
    "\n",
    "Q: Calculate the average sales per state.\n",
    "\n",
    "- Approach 1: Restructure the tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: (x[-3],(float(x[-1]),1)))\\\n",
    "    .reduceByKey(lambda (amount1,count1),(amount2,count2): \\\n",
    "        (amount1+amount2, count1+count2))\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note the argument unpacking we are doing in `reduceByKey` to name\n",
    "  the elements of the tuples.\n",
    "\n",
    "- Approach 2: Use `groupByKey`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean(iter):\n",
    "    total = 0.0; count = 0\n",
    "    for x in iter:\n",
    "        total += x; count += 1\n",
    "    return total/count\n",
    "\n",
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: (x[-3],float(x[-1])))\\\n",
    "    .groupByKey() \\\n",
    "    .map(lambda (state,iter): mean(iter))\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that we are using unpacking again.\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: What would be the disadvantage of not using unpacking?\n",
    "</summary>\n",
    "1. We will need to drill down into the elements.\n",
    "<br>\n",
    "2. The code will be harder to read.\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "Q: What are the pros and cons of `reduceByKey` vs `groupByKey`?\n",
    "</summary>\n",
    "1. `groupByKey` stores the values for particular key as an iterable.\n",
    "<br>\n",
    "2. This will take up space in memory or on disk.\n",
    "<br>\n",
    "3. `reduceByKey` therefore is more scalable.\n",
    "<br>\n",
    "4. However, `groupByKey` does not require associative reducer\n",
    "   operation.\n",
    "<br>\n",
    "5. For this reason `groupByKey` can be easier to program with.\n",
    "</details>\n",
    "\n",
    "\n",
    "Joins\n",
    "-----\n",
    "\n",
    "Q: Given a table of employees and locations find the cities that the\n",
    "employees live in.\n",
    "\n",
    "\n",
    "- The easiest way to do this is with a `join`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Employees: emp_id, loc_id, name\n",
    "employee_data = [\n",
    "    (101, 14, 'Alice'),\n",
    "    (102, 15, 'Bob'),\n",
    "    (103, 14, 'Chad'),\n",
    "    (104, 15, 'Jen'),\n",
    "    (105, 13, 'Dee') ]\n",
    "\n",
    "# Locations: loc_id, location\n",
    "location_data = [\n",
    "    (14, 'SF'),\n",
    "    (15, 'Seattle'),\n",
    "    (16, 'Portland')]\n",
    "\n",
    "employees = sc.parallelize(employee_data)\n",
    "locations = sc.parallelize(location_data)\n",
    "\n",
    "# Re-key employee records with loc_id\n",
    "employees2 = employees.map(lambda (emp_id,loc_id,name):(loc_id,name));\n",
    "\n",
    "# Now join.\n",
    "employees2.join(locations).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: How can we keep employees that don't have a valid location ID in\n",
    "the final result?\n",
    "</summary>\n",
    "1. Use `leftOuterJoin` to keep employees without location IDs.\n",
    "<br>\n",
    "2. Use `rightOuterJoin` to keep locations without employees. \n",
    "<br>\n",
    "3. Use `fullOuterJoin` to keep both.\n",
    "<br>\n",
    "</details>\n",
    "\n",
    "RDD Details\n",
    "===========\n",
    "\n",
    "RDD Statistics\n",
    "--------------\n",
    "\n",
    "Q: How would you calculate the mean, variance, and standard deviation of a sample\n",
    "produced by Python's `random()` function?\n",
    "\n",
    "- Create an RDD and apply the statistical actions to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 1000\n",
    "list = [random.random() for _ in xrange(count)]\n",
    "rdd = sc.parallelize(list)\n",
    "print rdd.mean()\n",
    "print rdd.variance()\n",
    "print rdd.stdev()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: What requirement does an RDD have to satisfy before you can apply\n",
    "these statistical actions to it? \n",
    "</summary>\n",
    "The RDD must consist of numeric elements.\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "Q: What is the advantage of using Spark vs Numpy to calculate mean or standard deviation?\n",
    "</summary>\n",
    "The calculation is distributed across different machines and will be\n",
    "more scalable.\n",
    "</details>\n",
    "\n",
    "RDD Laziness\n",
    "------------\n",
    "\n",
    "- Q: What is this Spark job doing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max = 10000000\n",
    "%time sc.parallelize(xrange(max)).map(lambda x:x+1).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q: How is the following job different from the previous one? How\n",
    "  long do you expect it to take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time sc.parallelize(xrange(max)).map(lambda x:x+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: Why did the second job complete so much faster?\n",
    "</summary>\n",
    "1. Because Spark is lazy. \n",
    "<br>\n",
    "2. Transformations produce new RDDs and do no operations on the data.\n",
    "<br>\n",
    "3. Nothing happens until an action is applied to an RDD.\n",
    "<br>\n",
    "4. An RDD is the *recipe* for a transformation, rather than the\n",
    "   *result* of the transformation.\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "Q: What is the benefit of keeping the recipe instead of the result of\n",
    "the action?\n",
    "</summary>\n",
    "1. It save memory.\n",
    "<br>\n",
    "2. It produces *resilience*. \n",
    "<br>\n",
    "3. If an RDD loses data on a machine, it always knows how to recompute it.\n",
    "</details>\n",
    "\n",
    "Writing Data\n",
    "------------\n",
    "\n",
    "Besides reading data Spark and also write data out to a file system.\n",
    "\n",
    "Q: Calculate the squares of integers from 1 to 100 and write them out\n",
    "to `squares.txt`.\n",
    "\n",
    "- Make sure `squares.txt` does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!if [ -e squares.txt ] ; then rm -rf squares.txt ; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create the RDD and then save it to `squares.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize(xrange(10))\n",
    "rdd2 = rdd1.map(lambda x: x*x)\n",
    "rdd2.saveAsTextFile('squares.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now look at the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cat squares.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Looks like the output is a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!ls -l squares.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets take a look at the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!for i in squares.txt/part-*; do echo $i; cat $i; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: What's going on? Why are there two files in the output directory?\n",
    "</summary>\n",
    "1. There were two threads that were processing the RDD.\n",
    "<br>\n",
    "2. The RDD was split up in two partitions (by default).\n",
    "<br>\n",
    "3. Each partition was processed in a different task.\n",
    "</details>\n",
    "\n",
    "Partitions\n",
    "----------\n",
    "\n",
    "Q: Can we control the number of partitions/tasks that Spark uses for\n",
    "processing data? Solve the same problem as above but this time with 5\n",
    "tasks.\n",
    "\n",
    "- Make sure `squares.txt` does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!if [ -e squares.txt ] ; then rm -rf squares.txt ; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create the RDD and then save it to `squares.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "partitions = 5\n",
    "rdd1 = sc.parallelize(xrange(10), partitions)\n",
    "rdd2 = rdd1.map(lambda x: x*x)\n",
    "rdd2.saveAsTextFile('squares.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now look at the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!ls -l squares.txt\n",
    "!for i in squares.txt/part-*; do echo $i; cat $i; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: How many partitions does Spark use by default?\n",
    "</summary>\n",
    "1. By default Spark uses 2 partitions.\n",
    "<br>\n",
    "2. If you read an HDFS file into an RDD Spark uses one partition per\n",
    "   block.\n",
    "<br>\n",
    "3. If you read a file into an RDD from S3 or some other source Spark\n",
    "   uses 1 partition per 32 MB of data.\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "Q: If I read a file that is 200 MB into an RDD, how many partitions will that have?\n",
    "</summary>\n",
    "1. If the file is on HDFS that will produce 2 partitions (each is 128\n",
    "   MB).\n",
    "<br>\n",
    "2. If the file is on S3 or some other file system it will produce 7\n",
    "   partitions.\n",
    "<br>\n",
    "3. You can also control the number of partitions by passing in an\n",
    "   additional argument into `textFile`.\n",
    "</details>\n",
    "\n",
    "Spark Terminology\n",
    "-----------------\n",
    "\n",
    "<img src=\"images/spark-cluster.png\">\n",
    "\n",
    "Term                   |Meaning\n",
    "----                   |-------\n",
    "Task                   |Single thread in an executor\n",
    "Partition              |Data processed by a single task\n",
    "Record                 |Records make up a partition that is processed by a single task\n",
    "\n",
    "Notes\n",
    "-----\n",
    "\n",
    "- Every Spark application gets executors when you create a new `SparkContext`.\n",
    "\n",
    "- You can specify how many cores to assign to each executor.\n",
    "\n",
    "- A core is equivalent to a thread.\n",
    "\n",
    "- The number of cores determine how many tasks can run concurrently on\n",
    "  an executor.\n",
    "\n",
    "- Each task corresponds to one partition.\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: Suppose you have 2 executors, each with 2 cores--so a total of 4\n",
    "cores. And you start a Spark job with 8 partitions. How many tasks\n",
    "will run concurrently?\n",
    "</summary>\n",
    "4 tasks will execute concurrently.\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "Q: What happens to the other partitions?\n",
    "</summary>\n",
    "1. The other partitions wait in queue until a task thread becomes\n",
    "available.\n",
    "<br>\n",
    "2. Think of cores as turnstile gates at a train station, and\n",
    "   partitions as people .\n",
    "<br>\n",
    "3. The number of turnstiles determine how many people can get through\n",
    "   at once.\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "Q: How many Spark jobs can you have in a Spark application?\n",
    "</summary>\n",
    "As many as you want.\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "Q: How many Spark applications and Spark jobs are in this IPython Notebook?\n",
    "</summary>\n",
    "1. There is one Spark application because there is one `SparkContext`.\n",
    "<br>\n",
    "2. There are as many Spark jobs as we have invoked actions on RDDs.\n",
    "</details>\n",
    "\n",
    "Stock Quotes\n",
    "------------\n",
    "\n",
    "Q: Find the date on which AAPL's stock price was the highest.\n",
    "\n",
    "Suppose you have stock market data from Yahoo! for AAPL from\n",
    "<http://finance.yahoo.com/q/hp?s=AAPL+Historical+Prices>. The data is\n",
    "in CSV format and has these values.\n",
    "\n",
    "Date        |Open    |High    |Low     |Close   |Volume      |Adj Close\n",
    "----        |----    |----    |---     |-----   |------      |---------\n",
    "11-18-2014  |113.94  |115.69  |113.89  |115.47  |44,200,300  |115.47\n",
    "11-17-2014  |114.27  |117.28  |113.30  |113.99  |46,746,700  |113.99\n",
    "\n",
    "Here is what the CSV looks like:\n",
    "    \n",
    "    csv = [\n",
    "      \"#Date,Open,High,Low,Close,Volume,Adj Close\\n\",\n",
    "      \"2014-11-18,113.94,115.69,113.89,115.47,44200300,115.47\\n\",\n",
    "      \"2014-11-17,114.27,117.28,113.30,113.99,46746700,113.99\\n\",\n",
    "    ]\n",
    "\n",
    "Lets find the date on which the price was the highest. \n",
    "\n",
    "\n",
    "<details><summary>\n",
    "Q: What two fields do we need to extract? \n",
    "</summary>\n",
    "1. *Date* and *Adj Close*.\n",
    "<br>\n",
    "2. We want to use *Adj Close* instead of *High* so our calculation is\n",
    "   not affected by stock splits.\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "Q: What field should we sort on?\n",
    "</summary>\n",
    "*Adj Close*\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "Q: What sequence of operations would we need to perform?\n",
    "</summary>\n",
    "1. Use `filter` to remove the header line.\n",
    "<br>\n",
    "2. Use `map` to split each row into fields.\n",
    "<br>\n",
    "3. Use `map` to extract *Adj Close* and *Date*.\n",
    "<br>\n",
    "4. Use `sortBy` to sort descending on *Adj Close*.\n",
    "<br>\n",
    "5. Use `take(1)` to get the highest value.\n",
    "</details>\n",
    "\n",
    "- Here is full source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv = [\n",
    "  \"#Date,Open,High,Low,Close,Volume,Adj Close\\n\",\n",
    "  \"2014-11-18,113.94,115.69,113.89,115.47,44200300,115.47\\n\",\n",
    "  \"2014-11-17,114.27,117.28,113.30,113.99,46746700,113.99\\n\",\n",
    "]\n",
    "sc.parallelize(csv) \\\n",
    "  .filter(lambda line: not line.startswith(\"#\")) \\\n",
    "  .map(lambda line: line.split(\",\")) \\\n",
    "  .map(lambda fields: (float(fields[-1]),fields[0])) \\\n",
    "  .sortBy(lambda (close, date): close, ascending=False) \\\n",
    "  .take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here is the program for finding the high of any stock that stores\n",
    "  the data in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import re\n",
    "\n",
    "def get_stock_high(symbol):\n",
    "  url = 'http://real-chart.finance.yahoo.com' + \\\n",
    "    '/table.csv?s='+symbol+'&g=d&ignore=.csv'\n",
    "  csv = urllib2.urlopen(url).read()\n",
    "  csv_lines = csv.split('\\n')\n",
    "  stock_rdd = sc.parallelize(csv_lines) \\\n",
    "    .filter(lambda line: re.match(r'\\d', line)) \\\n",
    "    .map(lambda line: line.split(\",\")) \\\n",
    "    .map(lambda fields: (float(fields[-1]),fields[0])) \\\n",
    "    .sortBy(lambda (close, date): close, ascending=False)\n",
    "  return stock_rdd.take(1)\n",
    "\n",
    "get_stock_high('AAPL')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes\n",
    "-----\n",
    "\n",
    "- Spark is high-level like Hive and Pig.\n",
    "\n",
    "- At the same time it does not invent a new language.\n",
    "\n",
    "- This allows it to leverage the ecosystem of tools that Python,\n",
    "  Scala, and Java provide.\n",
    "\n",
    "\n",
    "\n",
    "Caching and Persistence\n",
    "=======================\n",
    "\n",
    "RDD Caching\n",
    "-----------\n",
    "\n",
    "- Consider this Spark job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "num_count = 500*1000\n",
    "num_list = [random.random() for i in xrange(num_count)]\n",
    "rdd1 = sc.parallelize(num_list)\n",
    "rdd2 = rdd1.sortBy(lambda num: num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets time running `count()` on `rdd2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time rdd2.count()\n",
    "%time rdd2.count()\n",
    "%time rdd2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The RDD does no work until an action is called. And then when an\n",
    "  action is called it figures out the answer and then throws away all\n",
    "  the data.\n",
    "\n",
    "- If you have an RDD that you are going to reuse in your computation\n",
    "  you can use `cache()` to make Spark cache the RDD.\n",
    "\n",
    "- Lets cache it and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd2.cache()\n",
    "%time rdd2.count()\n",
    "%time rdd2.count()\n",
    "%time rdd2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Caching the RDD speeds up the job because the RDD does not have to\n",
    "  be computed from scratch again.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "\n",
    "- Calling `cache()` flips a flag on the RDD. \n",
    "\n",
    "- The data is not cached until an action is called.\n",
    "\n",
    "- You can uncache an RDD using `unpersist()`.\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: Will `unpersist` uncache the RDD immediately or does it wait for an\n",
    "action?\n",
    "</summary>\n",
    "It unpersists immediately.\n",
    "</details>\n",
    "\n",
    "Caching and Persistence\n",
    "-----------------------\n",
    "\n",
    "Q: Persist RDD to disk instead of caching it in memory.\n",
    "\n",
    "- You can cache RDDs at different levels.\n",
    "\n",
    "- Here is an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "rdd = sc.parallelize(xrange(100))\n",
    "rdd.persist(pyspark.StorageLevel.DISK_ONLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: Will the RDD be stored on disk at this point?\n",
    "</summary>\n",
    "No. It will get stored after we call an action.\n",
    "</details>\n",
    "\n",
    "Persistence Levels\n",
    "------------------\n",
    "\n",
    "Level                      |Meaning\n",
    "-----                      |-------\n",
    "`MEMORY_ONLY`              |Same as `cache()`\n",
    "`MEMORY_AND_DISK`          |Cache in memory then overflow to disk\n",
    "`MEMORY_AND_DISK_SER`      |Like above; in cache keep objects serialized instead of live \n",
    "`DISK_ONLY`                |Cache to disk not to memory\n",
    "\n",
    "Notes\n",
    "-----\n",
    "\n",
    "- `MEMORY_AND_DISK_SER` is a good compromise between the levels. \n",
    "\n",
    "- Fast, but not too expensive.\n",
    "\n",
    "- Make sure you unpersist when you don't need the RDD any more.\n",
    "\n",
    "\n",
    "Spark Performance\n",
    "=================\n",
    "\n",
    "Narrow and Wide Transformations\n",
    "-------------------------------\n",
    "\n",
    "- Spark transformations are *narrow* if each RDD has one unique child\n",
    "  past the transformation.\n",
    "\n",
    "- Spark transformations are *wide* if each RDD can have multiple\n",
    "  children past the transformation.\n",
    "\n",
    "- Narrow transformations are map-like, while wide transformations are\n",
    "  reduce-like.\n",
    "\n",
    "- Narrow transformations are faster because they do move data between\n",
    "  executors, while wide transformations are slower.\n",
    " \n",
    "Repartitioning\n",
    "--------------\n",
    "\n",
    "- Over time partitions can get skewed. \n",
    "\n",
    "- Or you might have less data or more data than you started with.\n",
    "\n",
    "- You can rebalance your partitions using `repartition` or `coalesce`.\n",
    "\n",
    "- `coalesce` is narrow while `repartition` is wide.\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Between `coalesce` and `repartition` which one is faster? Which one is\n",
    "more effective?\n",
    "</summary>\n",
    "1. `coalesce` is narrow so it is faster. \n",
    "<br>\n",
    "2. However, it only combines partitions and does not shuffle them.\n",
    "<br>\n",
    "3. `repartition` is wide but it partitions more effectively because it\n",
    "   reshuffles the records.\n",
    "</details>\n",
    "\n",
    "Misc\n",
    "====\n",
    "\n",
    "Amazon S3\n",
    "---------\n",
    "\n",
    "- *\"s3:\" URLs break when Secret Key contains a slash, even if encoded*\n",
    "    <https://issues.apache.org/jira/browse/HADOOP-3733>\n",
    "\n",
    "- *Spark 1.3.1 / Hadoop 2.6 prebuilt pacakge has broken S3 filesystem access*\n",
    "    <https://issues.apache.org/jira/browse/SPARK-7442>\n",
    "\n",
    "<!--\n",
    "\n",
    "Broadcast Variables\n",
    "\n",
    "Accumulators\n",
    "\n",
    "Checkpoint\n",
    "\n",
    "S3 key issue\n",
    "\n",
    "2.6\n",
    "\n",
    "-->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
